{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cbc30a9b-7f61-4250-89a4-7ed4ae79765f",
   "metadata": {},
   "source": [
    "# 02_Data_Preprocessing.ipynb\n",
    "\n",
    "## Descripción General\n",
    "Este cuaderno implementa el pipeline de ingeniería de datos (ETL) necesario para preparar el dataset CIFAR-10 para el entrenamiento de modelos SOTA.\n",
    "\n",
    "## Objetivos\n",
    "1. **Extracción:** Descargar el dataset CIFAR-10 oficial.\n",
    "2. **Transformación:**\n",
    "   - Filtrar únicamente las clases de interés: *Dog, Automobile, Bird*.\n",
    "   - Normalizar tipos de datos a `float32` para compatibilidad con TensorFlow 2.x.\n",
    "   - Realizar codificación One-Hot manual para asegurar el orden de clases.\n",
    "   - Dividir los datos en conjuntos de Entrenamiento (Train) y Validación (Val).\n",
    "3. **Carga (Persistencia):** Guardar los tensores procesados en formato `.npy` para maximizar la velocidad de carga en el entrenamiento.\n",
    "\n",
    "## Configuración del Pipeline\n",
    "- **Input:** CIFAR-10 (TensorFlow Datasets).\n",
    "- **Output:** Archivos `.npy` (X_train, y_train, X_val, y_val, X_test, y_test).\n",
    "- **Resolución:** 224x224 (Escalado posterior)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "056e5de4-931f-4284-ad3a-24411fb35474",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "import os\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Verificacion de hardware disponible\n",
    "print(f\"Dispositivos GPU detectados: {tf.config.list_physical_devices('GPU')}\")\n",
    "\n",
    "# ============================================================================\n",
    "# 0. CONFIGURACION GLOBAL\n",
    "# ============================================================================\n",
    "\n",
    "CONFIG = {\n",
    "    'IMG_SIZE': 224,        # Resolucion objetivo para modelos ViT/ConvNeXt\n",
    "    'BATCH_SIZE': 32,       # Tamano de lote estandar\n",
    "    'NUM_CLASSES': 3,       # Clases activas\n",
    "    'SEED': 42,             # Semilla para reproducibilidad\n",
    "    'VALIDATION_SPLIT': 0.2 # 20% para validacion\n",
    "}\n",
    "\n",
    "# Mapeo de indices originales de CIFAR-10\n",
    "# 1: Automobile, 2: Bird, 5: Dog\n",
    "CLASS_MAPPING = {\n",
    "    'dog': 5,\n",
    "    'automobile': 1,\n",
    "    'bird': 2,\n",
    "}\n",
    "\n",
    "# Orden estricto de las clases para el entrenamiento\n",
    "CLASS_NAMES = ['dog', 'automobile', 'bird']\n",
    "\n",
    "# Creacion de estructura de directorios\n",
    "os.makedirs('/tf/notebooks/data', exist_ok=True)\n",
    "os.makedirs('/tf/notebooks/outputs', exist_ok=True)\n",
    "\n",
    "# Persistencia de la configuracion\n",
    "with open('/tf/notebooks/config.json', 'w') as f:\n",
    "    json.dump(CONFIG, f, indent=4)\n",
    "\n",
    "print(\"Configuracion del sistema inicializada y guardada.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e28444f2-a560-42c2-ab40-94cea5f0e438",
   "metadata": {},
   "source": [
    "## 1. Extracción de Datos\n",
    "Se descarga el dataset CIFAR-10 completo desde los repositorios de TensorFlow Datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6746dc52-a512-45ca-8468-a97b02f3b313",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# 1. CARGA DE DATOS (SOURCE)\n",
    "# ============================================================================\n",
    "\n",
    "print(\"Iniciando descarga de CIFAR-10...\")\n",
    "\n",
    "(ds_train, ds_test), ds_info = tfds.load(\n",
    "    'cifar10',\n",
    "    split=['train', 'test'],\n",
    "    as_supervised=True,\n",
    "    with_info=True,\n",
    "    shuffle_files=True\n",
    ")\n",
    "\n",
    "print(\"Descarga completada exitosamente.\")\n",
    "print(f\"Registros de Entrenamiento: {ds_info.splits['train'].num_examples}\")\n",
    "print(f\"Registros de Prueba: {ds_info.splits['test'].num_examples}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b38f054e-f366-4251-b970-64cedc8292ae",
   "metadata": {},
   "source": [
    "## 2. Filtrado y Transformación\n",
    "En esta etapa se aplica la lógica de negocio para seleccionar solo las clases requeridas.\n",
    "Se realiza un casteo explícito a `float32` para evitar conflictos de tipos durante operaciones matriciales complejas (como MixUp) en TensorFlow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "880b44f6-88eb-44c2-b137-2551be781b08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# 2. PROCESAMIENTO Y FILTRADO\n",
    "# ============================================================================\n",
    "\n",
    "def extract_filtered_data(dataset, class_ids):\n",
    "    \"\"\"\n",
    "    Filtra el dataset para conservar solo las clases especificas,\n",
    "    aplica One-Hot Encoding manual y asegura tipos float32.\n",
    "\n",
    "    Args:\n",
    "        dataset: Objeto TFDS.\n",
    "        class_ids: Lista de enteros con los IDs de CIFAR-10 a conservar.\n",
    "\n",
    "    Returns:\n",
    "        np.array: Imagenes filtradas (uint8).\n",
    "        np.array: Etiquetas One-Hot (float32).\n",
    "    \"\"\"\n",
    "    images = []\n",
    "    labels = []\n",
    "    \n",
    "    print(\"Procesando particion de datos...\")\n",
    "    \n",
    "    # Iteracion sobre el dataset para filtrado manual\n",
    "    for img, label in tqdm(dataset, desc=\"Filtrando Clases\"):\n",
    "        label_val = label.numpy()\n",
    "        \n",
    "        if label_val in class_ids:\n",
    "            # Las imagenes se mantienen en uint8 para optimizar almacenamiento en disco\n",
    "            images.append(img.numpy()) \n",
    "            \n",
    "            # One-hot encoding manual para garantizar el orden estricto:\n",
    "            # [Dog, Automobile, Bird]\n",
    "            if label_val == CLASS_MAPPING['dog']:\n",
    "                labels.append([1., 0., 0.])\n",
    "            elif label_val == CLASS_MAPPING['automobile']:\n",
    "                labels.append([0., 1., 0.])\n",
    "            elif label_val == CLASS_MAPPING['bird']:\n",
    "                labels.append([0., 0., 1.])\n",
    "    \n",
    "    # Casting critico a float32 para las etiquetas\n",
    "    return np.array(images), np.array(labels, dtype=np.float32)\n",
    "\n",
    "# Ejecucion del filtrado\n",
    "target_class_ids = list(CLASS_MAPPING.values())\n",
    "\n",
    "print(\"Iniciando filtrado de Training Set...\")\n",
    "X_train_full, y_train_full = extract_filtered_data(ds_train, target_class_ids)\n",
    "\n",
    "print(\"Iniciando filtrado de Test Set...\")\n",
    "X_test, y_test = extract_filtered_data(ds_test, target_class_ids)\n",
    "\n",
    "print(\"\\nResumen de dimensiones post-procesamiento:\")\n",
    "print(f\"X_train shape: {X_train_full.shape} (Dtype: {X_train_full.dtype})\")\n",
    "print(f\"y_train shape: {y_train_full.shape} (Dtype: {y_train_full.dtype})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30717d30-a4ea-41d3-8e6f-8af2d479683b",
   "metadata": {},
   "source": [
    "## 3. Verificación de Calidad de Datos (QA)\n",
    "Se realiza una inspección visual rápida y un conteo de distribución de clases para asegurar que el proceso de filtrado no ha introducido sesgos o errores de etiquetado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66b05ad3-7372-4693-91c7-52122d0bea0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# 3. VERIFICACION DE DATOS (QA)\n",
    "# ============================================================================\n",
    "\n",
    "# Calculo de distribucion de clases\n",
    "train_counts = y_train_full.sum(axis=0)\n",
    "test_counts = y_test.sum(axis=0)\n",
    "\n",
    "print(\"Distribucion de clases resultante:\")\n",
    "print(f\"Train: {dict(zip(CLASS_NAMES, train_counts))}\")\n",
    "print(f\"Test:  {dict(zip(CLASS_NAMES, test_counts))}\")\n",
    "\n",
    "# Generacion de grilla de muestras para inspeccion visual\n",
    "fig, axes = plt.subplots(3, 5, figsize=(15, 9))\n",
    "fig.suptitle('Muestras del Dataset Filtrado (Verificacion)', fontsize=16)\n",
    "\n",
    "for idx in range(15):\n",
    "    ax = axes[idx // 5, idx % 5]\n",
    "    ax.imshow(X_train_full[idx])\n",
    "    # Decodificacion de One-Hot a nombre de clase\n",
    "    class_name = CLASS_NAMES[np.argmax(y_train_full[idx])]\n",
    "    ax.set_title(f\"{class_name}\", fontsize=10)\n",
    "    ax.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('/tf/notebooks/outputs/01_dataset_qa_samples.png', dpi=150)\n",
    "print(\"Imagen de verificacion guardada en outputs/01_dataset_qa_samples.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af3a44ee-d0c2-4d2d-a430-e9e13fc5ee4e",
   "metadata": {},
   "source": [
    "## 4. División del Dataset\n",
    "Separación del conjunto de entrenamiento en **Entrenamiento** y **Validación** para monitorear el rendimiento del modelo y evitar el sobreajuste (overfitting). Se utiliza una semilla fija (`SEED=42`) para garantizar la reproducibilidad de los experimentos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2ea316b-be00-4a6a-8f91-e1af7bf09c67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# 4. SPLIT DE ENTRENAMIENTO Y VALIDACION\n",
    "# ============================================================================\n",
    "\n",
    "# Calculo de indices de corte\n",
    "val_count = int(len(X_train_full) * CONFIG['VALIDATION_SPLIT'])\n",
    "train_count = len(X_train_full) - val_count\n",
    "\n",
    "# Mezcla aleatoria de indices (Shuffling)\n",
    "np.random.seed(CONFIG['SEED'])\n",
    "indices = np.random.permutation(len(X_train_full))\n",
    "\n",
    "train_indices = indices[:train_count]\n",
    "val_indices = indices[train_count:]\n",
    "\n",
    "# Asignacion de datos basada en indices\n",
    "X_train = X_train_full[train_indices]\n",
    "y_train = y_train_full[train_indices]\n",
    "\n",
    "X_val = X_train_full[val_indices]\n",
    "y_val = y_train_full[val_indices]\n",
    "\n",
    "print(\"Split completado:\")\n",
    "print(f\"Set de Entrenamiento: {len(X_train)} muestras\")\n",
    "print(f\"Set de Validacion:    {len(X_val)} muestras\")\n",
    "print(f\"Set de Prueba:        {len(X_test)} muestras\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c0f3fd4-cb0e-479c-ac8d-0ca1adaaab57",
   "metadata": {},
   "source": [
    "## 5. Almacenamiento (Persistencia)\n",
    "Guardado de los arrays procesados en formato binario `.npy`. Este formato es altamente eficiente para lectura/escritura en NumPy y TensorFlow, reduciendo el tiempo de carga en el cuaderno de entrenamiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93f2e859-b13c-457f-b8ff-a2955051f940",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# 5. PERSISTENCIA DE DATOS\n",
    "# ============================================================================\n",
    "\n",
    "print(\"Guardando datasets procesados en disco...\")\n",
    "\n",
    "# Guardado en formato NumPy (.npy)\n",
    "np.save('/tf/notebooks/data/X_train.npy', X_train)\n",
    "np.save('/tf/notebooks/data/y_train.npy', y_train)\n",
    "\n",
    "np.save('/tf/notebooks/data/X_val.npy', X_val)\n",
    "np.save('/tf/notebooks/data/y_val.npy', y_val)\n",
    "\n",
    "np.save('/tf/notebooks/data/X_test.npy', X_test)\n",
    "np.save('/tf/notebooks/data/y_test.npy', y_test)\n",
    "\n",
    "# Guardado de metadatos del dataset para referencia\n",
    "dataset_metadata = {\n",
    "    'train_samples': len(X_train),\n",
    "    'val_samples': len(X_val),\n",
    "    'test_samples': len(X_test),\n",
    "    'class_distribution': {\n",
    "        'train': train_counts.tolist(),\n",
    "        'test': test_counts.tolist()\n",
    "    },\n",
    "    'data_specs': {\n",
    "        'X_dtype': str(X_train.dtype),\n",
    "        'y_dtype': str(y_train.dtype)\n",
    "    },\n",
    "    **CONFIG\n",
    "}\n",
    "\n",
    "with open('/tf/notebooks/data/dataset_metadata.json', 'w') as f:\n",
    "    json.dump(dataset_metadata, f, indent=4)\n",
    "\n",
    "print(\"Pipeline finalizado exitosamente.\")\n",
    "print(\"Archivos generados en: /tf/notebooks/data/\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
