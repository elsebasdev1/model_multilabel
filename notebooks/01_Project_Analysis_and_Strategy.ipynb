{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fd9c80c0-8fbc-44c9-81f0-62ae423ce423",
   "metadata": {},
   "source": [
    "# 01_Project_Analysis_and_Strategy.ipynb\n",
    "\n",
    "## 1. Definición del Proyecto\n",
    "Este proyecto tiene como objetivo desarrollar un sistema de visión artificial **SOTA (State-of-the-Art)** capaz de realizar clasificación **Multi-Label** en entornos no controlados. A diferencia de los clasificadores tradicionales que seleccionan una única clase dominante (Softmax), este sistema evaluará la presencia independiente de múltiples objetos (Sigmoid) en una misma escena.\n",
    "\n",
    "### Casos de Uso\n",
    "El sistema está diseñado para identificar simultáneamente tres entidades clave en imágenes del mundo real:\n",
    "1.  **Automobile** (Vehículos)\n",
    "2.  **Bird** (Fauna aérea)\n",
    "3.  **Dog** (Fauna doméstica)\n",
    "\n",
    "## 2. Selección del Dataset: CIFAR-10\n",
    "Se ha seleccionado **CIFAR-10** como dataset base para el entrenamiento por las siguientes razones técnicas:\n",
    "* **Benchmark Estándar:** Es uno de los datasets más auditados en la literatura científica, lo que permite comparar métricas de rendimiento con papers actuales.\n",
    "* **Volumen de Datos:** Contiene 60,000 imágenes, suficiente para garantizar la convergencia del modelo sin requerir terabytes de almacenamiento.\n",
    "* **Balance de Clases:** Presenta una distribución uniforme perfecta (6,000 imágenes por clase), eliminando la necesidad de técnicas de re-muestreo (Oversampling/Undersampling) en esta fase inicial.\n",
    "\n",
    "**Nota sobre la Resolución:** Aunque las imágenes nativas son de 32x32 píxeles, se implementará un pipeline de **Super-Resolución Bicúbica** (Upscaling) a 224x224 píxeles para ser compatibles con la arquitectura ConvNeXt.\n",
    "\n",
    "## 3. Selección del Modelo: ConvNeXt Base\n",
    "Para la arquitectura del modelo, se ha descartado el uso de CNNs tradicionales (como ResNet) en favor de **ConvNeXt Base**.\n",
    "\n",
    "### Justificación Técnica:\n",
    "* **Arquitectura Híbrida:** ConvNeXt moderniza las Redes Convolucionales adoptando decisiones de diseño de los **Vision Transformers (ViT)** (como el uso de parches no superpuestos, bloques invertidos y Layer Normalization), pero manteniendo la eficiencia inductiva de las CNNs.\n",
    "* **Rendimiento/Coste:** Ofrece una precisión comparable a ViT-B (Vision Transformer Base) pero es más fácil de entrenar y converger con datasets de tamaño mediano como el nuestro.\n",
    "* **Feature Extraction:** Al utilizar pesos pre-entrenados en **ImageNet-1k**, aprovechamos el aprendizaje por transferencia para extraer características complejas (texturas, formas) que no sería posible aprender desde cero con CIFAR-10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb55ec77-b10c-4c5e-a663-b06f546b16e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "# Configuracion de estilo para graficos profesionales\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "sns.set_context(\"notebook\", font_scale=1.2)\n",
    "\n",
    "print(f\"TensorFlow Version: {tf.__version__}\")\n",
    "print(f\"Dispositivos disponibles: {tf.config.list_physical_devices()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9b80889-29da-4ac7-902c-88e4a530c4cf",
   "metadata": {},
   "source": [
    "## 4. Exploración de Datos (EDA)\n",
    "En esta sección cargamos una muestra del dataset crudo para verificar la integridad de las clases seleccionadas y analizar la variabilidad visual."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2a6929b-ac17-4a0c-9ed4-e339bfa3dcf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carga temporal del dataset (solo para analisis)\n",
    "ds_raw, info = tfds.load('cifar10', split='train', with_info=True, as_supervised=True)\n",
    "\n",
    "# Definicion de clases objetivo (Indices CIFAR-10)\n",
    "TARGET_CLASSES = {\n",
    "    1: 'Automobile',\n",
    "    2: 'Bird',\n",
    "    5: 'Dog'\n",
    "}\n",
    "\n",
    "def get_samples_by_class(dataset, target_class_idx, num_samples=5):\n",
    "    \"\"\"\n",
    "    Extrae n muestras especificas de una clase para visualizacion.\n",
    "    \"\"\"\n",
    "    samples = []\n",
    "    for img, label in dataset:\n",
    "        if label.numpy() == target_class_idx:\n",
    "            samples.append(img.numpy())\n",
    "        if len(samples) == num_samples:\n",
    "            break\n",
    "    return samples\n",
    "\n",
    "# Visualizacion\n",
    "fig, axes = plt.subplots(3, 5, figsize=(15, 10))\n",
    "fig.suptitle('Muestreo Aleatorio por Clase (Resolución Nativa 32x32)', fontsize=16)\n",
    "\n",
    "for i, (class_idx, class_name) in enumerate(TARGET_CLASSES.items()):\n",
    "    samples = get_samples_by_class(ds_raw, class_idx, 5)\n",
    "    for j, img in enumerate(samples):\n",
    "        ax = axes[i, j]\n",
    "        ax.imshow(img)\n",
    "        if j == 0:\n",
    "            ax.set_ylabel(class_name, fontsize=14, fontweight='bold')\n",
    "        ax.grid(False)\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "918e7709-8e01-4278-8be3-b0163569ee79",
   "metadata": {},
   "source": [
    "## 5. Análisis de Distribución de Píxeles\n",
    "Analizamos el histograma de intensidad de píxeles. Esto es crucial para determinar la estrategia de normalización.\n",
    "* Si los datos están concentrados o sesgados, la convergencia del modelo será lenta.\n",
    "* Validamos que los valores estén en el rango `[0, 255]` para confirmar la necesidad de escalado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4ab707a-a61f-41bb-9be1-2c7157c8239d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extraccion de una muestra grande para estadistica\n",
    "sample_images = []\n",
    "for img, label in ds_raw.take(1000): # Tomamos 1000 imagenes aleatorias\n",
    "    if label.numpy() in TARGET_CLASSES:\n",
    "        sample_images.append(img.numpy())\n",
    "\n",
    "sample_stack = np.array(sample_images)\n",
    "\n",
    "# Grafico de distribucion\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.histplot(sample_stack.ravel(), bins=50, color='darkslategray', kde=True)\n",
    "plt.title('Distribución Global de Intensidad de Píxeles (RGB)')\n",
    "plt.xlabel('Valor de Píxel (0-255)')\n",
    "plt.ylabel('Frecuencia (Escala Logarítmica)')\n",
    "plt.yscale('log')\n",
    "plt.xlim(0, 255)\n",
    "plt.show()\n",
    "\n",
    "print(f\"Estadísticas de la muestra:\")\n",
    "print(f\"Min: {sample_stack.min()}, Max: {sample_stack.max()}\")\n",
    "print(f\"Media: {sample_stack.mean():.2f}, Desviación Std: {sample_stack.std():.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84342828-d534-404e-8a39-2148e4f276fa",
   "metadata": {},
   "source": [
    "## 6. Conclusiones y Estrategia de Procesamiento\n",
    "\n",
    "Basado en el análisis exploratorio, se definen las siguientes directrices para el **Cuaderno 02 (Preprocesamiento)**:\n",
    "\n",
    "1.  **Filtrado Estricto:** El dataset contiene 10 clases. Se implementará un filtro lógico para descartar las 7 clases irrelevantes y conservar solo *Automobile, Bird, Dog*.\n",
    "2.  **Pipeline de Normalización:**\n",
    "    - Los datos crudos están en rango `uint8` [0, 255].\n",
    "    - **Estrategia:** ConvNeXt requiere preprocesamiento específico. Se mantendrán los inputs en rango 0-255 durante la etapa de almacenamiento, pero se aplicará la función `preprocess_input` de Keras dinámicamente o una capa de `Rescaling` dentro del modelo.\n",
    "3.  **Aumento de Datos (Data Augmentation):**\n",
    "    - Dada la baja resolución original, el modelo podría sobreajustarse a texturas pixeladas.\n",
    "    - Se aplicará `RandomFlip`, `RandomRotation` y, crucialmente, **MixUp** durante el entrenamiento para forzar al modelo a aprender características robustas y no memorizar píxeles.\n",
    "4.  **Estrategia Multi-Label:**\n",
    "    - Aunque las etiquetas de CIFAR-10 son mutuamente excluyentes (Single-Label), configuraremos la capa final del modelo con **3 neuronas y activación Sigmoid** (en lugar de Softmax). Esto permitirá que el modelo, en producción, pueda asignar probabilidades altas a un Perro y un Auto simultáneamente si aparecen en la misma imagen."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
